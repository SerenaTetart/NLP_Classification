# NLP_Classification

## Table of contents
* [General info](#general-info)
* [Requirements](#requirements)
* [Project 1: SMS Spam detection](#project-1---SMS-Spam-detection)
* [Project 2: Fake news detection](#Project-2---Fake-news-detection)
* [Project 3: Toxic comments detection/classification](#Project-3---Toxic-comments-detectionclassification)

## General info
In this repository you'll learn how to classify text based on multiple models such as: Naive Baye, LSTM, Transformers (Bert) and One vs All

## Requirements

The basic libraries for machine learning and deep learning ðŸ˜ƒ

Libraries:
* Tensorflow
* Keras
* Pytorch
* Scikit-learn
* Pandas
* Numpy
* Matplotlib
* Transformers (Hugging face)
* Unidecode
* nltk

## Project 1 - SMS Spam detection

## Project 2 - Fake news detection

## Project 3 - Toxic comments detection

For this last project I am using the dataset from kaggle: <a href='https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification'>Jigsaw Unintended Bias in Toxicity Classification</a>

This project needs some changes in order to be perfect but we can already predict 'normal' and 'toxic' data very well.

Using Naive Baye we have pretty good results for respectively normal and toxic data with 96.80% and 75.08% accuracy.

<p align="center">
<img src="https://user-images.githubusercontent.com/65224852/150573359-655195b0-57a8-4c52-8d54-28edfaf0fce2.PNG">
</p>
